{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "836a887a",
   "metadata": {},
   "source": [
    "# Info \n",
    "\n",
    "This script is mean to perform the lineage reconstruction of the different explant trees using the greedy and neighbor joining methods from cassiopeia.\n",
    "\n",
    "The hybrid method was completed using seperate scripts due to the length it took to run each hybrid reconstruction step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b2fb478",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cassiopeia as cas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80214f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = '/Genomics/chanlab/blaw/TLS/data/explant/lineage/3_lineage_reconstruction/'\n",
    "alleleTable_filepath = '/Genomics/chanlab/blaw/TLS/data/explant/lineage/2_add_metadata/merged_allele_table_with_metadata.txt'\n",
    "\n",
    "barcodes = ['Bar1', 'Bar2', 'Bar3', 'Bar4', 'Bar5', 'Bar6']\n",
    "\n",
    "# Read in the allele table\n",
    "allele_table = pd.read_csv(alleleTable_filepath, sep='\\t')\n",
    "allele_table.drop(columns = ['Unnamed: 0'], inplace = True)\n",
    "\n",
    "# read in the cell state tables\n",
    "explant_cell_state = pd.read_csv('/Genomics/chanlab/blaw/TLS/metadata/AM-RNA-929_cellBC_cellState.tsv', sep='\\t')\n",
    "outgrowth_1_cell_state = pd.read_csv('/Genomics/chanlab/blaw/TLS/metadata/AM-RNA-930_cellBC_cellState.tsv', sep='\\t')\n",
    "outgrowth_2_cell_state = pd.read_csv('/Genomics/chanlab/blaw/TLS/metadata/AM-RNA-931_cellBC_cellState.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b288907",
   "metadata": {},
   "source": [
    "# Organize and merge the cell state tables with the allele table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de97ee08",
   "metadata": {},
   "outputs": [],
   "source": [
    "explant_cell_state['cellBC'] = ['Tracer_Explant_' + i[:-2] for i in explant_cell_state['cellBC']]\n",
    "\n",
    "outgrowth_1_cell_state['cellBC'] = [i[:-2] for i in outgrowth_1_cell_state['cellBC']]\n",
    "outgrowth_2_cell_state['cellBC'] = [i[:-2] for i in outgrowth_2_cell_state['cellBC']]\n",
    "\n",
    "explant_cell_state.set_index('cellBC', inplace = True)\n",
    "outgrowth_1_cell_state.set_index('cellBC', inplace = True)\n",
    "outgrowth_2_cell_state.set_index('cellBC', inplace = True)\n",
    "\n",
    "allele_table['cell_state'] = ''\n",
    "cell_states = []\n",
    "\n",
    "for i in allele_table.index:\n",
    "    cellBC = allele_table.loc[i, 'cellBC']\n",
    "    \n",
    "    if cellBC.startswith('Tracer_Explant'):\n",
    "        cell_states.append(explant_cell_state.loc[cellBC, 'cell_state'])\n",
    "    elif cellBC.startswith('Tracer_Outgrowth_1'):\n",
    "        cell_states.append(outgrowth_1_cell_state.loc[cellBC, 'cell_state'])\n",
    "    elif cellBC.startswith('Tracer_Outgrowth_2'):\n",
    "        cell_states.append(outgrowth_2_cell_state.loc[cellBC, 'cell_state'])\n",
    "        \n",
    "allele_table['cell_state'] = cell_states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cef959",
   "metadata": {},
   "source": [
    "# Seperate edited and unedited cellBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f001a78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any cell that is not edited\n",
    "edited_cellBC = set()\n",
    "unedited_cellBC = set()\n",
    "\n",
    "for cellBC in allele_table['cellBC'].unique():\n",
    "    bad = True\n",
    "    for i in allele_table[allele_table['cellBC'] == cellBC]['allele']:\n",
    "        if i != '[None][None][None]':\n",
    "            bad = False\n",
    "    \n",
    "    if bad:\n",
    "        unedited_cellBC.add(cellBC)\n",
    "    else:\n",
    "        edited_cellBC.add(cellBC)\n",
    "\n",
    "tableFiltered = allele_table[allele_table['cellBC'].isin(edited_cellBC)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9744af7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tableFiltered.to_csv(output + 'allele_table_filtered.txt', sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4e2187",
   "metadata": {},
   "outputs": [],
   "source": [
    "tableFiltered = pd.read_csv(output + 'allele_table_filtered.txt', sep = '\\t', index_col = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae66d25",
   "metadata": {},
   "source": [
    "# Generate lineage and prior tables for cassiopeia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf3b73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the lineage table\n",
    "lineage_table = cas.pp.convert_alleletable_to_lineage_profile(tableFiltered)\n",
    "lineage_table.to_csv(output + 'Total_Explant_Lineage_Table.txt', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c373fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any rows with NaN alleles, since they disrupt Cassiopeia\n",
    "def remove_NaN(table):\n",
    "    '''\n",
    "    input:\n",
    "        table - an allele_table that contains an r1, r2, and r3 cute site column\n",
    "    output:\n",
    "        returns a new table with any rows that contain NaN removed\n",
    "    '''\n",
    "    table = table[table['r1'].isnull() == False]\n",
    "    table = table[table['r2'].isnull() == False]\n",
    "    table = table[table['r3'].isnull() == False]\n",
    "    \n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92972a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculate the total indels using allele_tables from TLS1, TLS2, and TLSCL combined\n",
    "TLSCL_filepath = '/Genomics/chanlab/blaw/TLS/sandbox/AM-DNA-258/2_add_metadata/allele_table_multiSeq+lenti.txt'\n",
    "TLS1_filepath = '/Genomics/chanlab/blaw/TLS/sandbox/AM-DNA-097/1_preprocessing/allele_table.txt'\n",
    "TLS2_filepath = '/Genomics/chanlab/blaw/TLS/sandbox/AM-DNA-098/1_preprocessing/allele_table.txt'\n",
    "explant_filepath = '/Genomics/chanlab/blaw/TLS/sandbox/explant/2_add_metadata/merged_allele_table_with_metadata.txt'\n",
    "\n",
    "TLSCL_allele = pd.read_csv(TLSCL_filepath, sep='\\t')\n",
    "TLS1_allele = pd.read_csv(TLS1_filepath, sep='\\t')\n",
    "TLS2_allele = pd.read_csv(TLS2_filepath, sep='\\t')\n",
    "explant_allele = pd.read_csv(explant_filepath, sep = '\\t')\n",
    "\n",
    "# Combine all three datasets\n",
    "TLSCL_allele = TLSCL_allele[(TLSCL_allele['finalCalls'] != 'Doublet') & (TLSCL_allele['finalCalls'] != 'Negative')]\n",
    "TLS1_allele['finalCalls'] = 'Bar25'\n",
    "TLS2_allele['finalCalls'] = 'Bar26'\n",
    "explant_allele['finalCalls'] = ['explant_' + i for i in explant_allele['finalCalls']]\n",
    "total_table = pd.concat([TLSCL_allele, TLS1_allele, TLS2_allele, explant_allele])\n",
    "total_table = remove_NaN(total_table)\n",
    "\n",
    "# Calculate the priors using each intBC / Multiseq Barcode as a different indel instance\n",
    "total_priors = cas.pp.compute_empirical_indel_priors(total_table, grouping_variables=['intBC', 'finalCalls'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c711e45",
   "metadata": {},
   "source": [
    "# Lineage reconstruction\n",
    "For each multiseq dataset, I will generate trees for the 120, 144, 120_144 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "901f3f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_tree_pipeline(table, name, total_priors, output_loc):\n",
    "    character_matrix, priors, state_2_indel = cas.pp.convert_alleletable_to_character_matrix(table,\n",
    "                                                                                         allele_rep_thresh = 1,\n",
    "                                                                                         mutation_priors = total_priors)  \n",
    "\n",
    "    character_matrix.to_csv(output_loc + 'greedy/' + name + '_character_matrix.txt', sep='\\t')\n",
    "    \n",
    "    with open(output_loc + 'greedy/' + name + '_priors.pickle', 'wb') as handle:\n",
    "        pickle.dump(priors, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open(output_loc + 'greedy/' + name + '_state_2_indel.pickle', 'wb') as handle:\n",
    "        pickle.dump(state_2_indel, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    cas_tree = cas.data.CassiopeiaTree(character_matrix=character_matrix, priors=priors)\n",
    "    print(cas_tree.n_cell, cas_tree.n_character, '\\n')\n",
    "    \n",
    "    missing_proportion = (character_matrix == -1).sum(axis=0) / character_matrix.shape[0]\n",
    "    uncut_proportion = (character_matrix == 0).sum(axis=0) / character_matrix.shape[0]\n",
    "    n_unique_states = character_matrix.apply(lambda x: len(np.unique(x[(x != 0) & (x != -1)])), axis=0)\n",
    "\n",
    "    cell_meta = table.groupby('cellBC').agg({\"intBC\": 'nunique', 'UMI': 'sum', 'cell_state': 'unique', 'Timepoint': 'unique', 'orig.ident':'unique'})\n",
    "    character_meta = pd.DataFrame([missing_proportion, uncut_proportion, n_unique_states], index = ['missing_prop', 'uncut_prop', 'n_unique_states']).T\n",
    "\n",
    "    cas_tree.cell_meta = cell_meta\n",
    "    cas_tree.character_meta = character_meta\n",
    "    cas_tree.cell_meta.to_csv(output_loc + 'greedy/' + name + '_metadata.txt', sep='\\t')\n",
    "    \n",
    "    cas_tree.cell_meta['cell_state'] = cas_tree.cell_meta['cell_state'].astype('str')\n",
    "    \n",
    "    vanilla_greedy = cas.solver.VanillaGreedySolver()\n",
    "    vanilla_greedy.solve(cas_tree)\n",
    "    \n",
    "    newickFile = open(output_loc + 'greedy/' + name + '_greedy_newick.txt', 'wt')\n",
    "    newickFile.write(cas_tree.get_newick())\n",
    "    newickFile.close()\n",
    "    \n",
    "    cas_tree.collapse_mutationless_edges(infer_ancestral_characters=True)\n",
    "    \n",
    "    noMutationlessEdgesFile = open(output_loc + 'greedy/' + name + '_greedy_newick_noMutationlessEdges.txt', 'wt')\n",
    "    noMutationlessEdgesFile.write(cas_tree.get_newick())\n",
    "    noMutationlessEdgesFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4d4e562",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NJ_tree_pipeline(table, name, total_priors, output_loc):\n",
    "    character_matrix, priors, state_2_indel = cas.pp.convert_alleletable_to_character_matrix(table,\n",
    "                                                                                         allele_rep_thresh = 1,\n",
    "                                                                                         mutation_priors = total_priors)  \n",
    "\n",
    "    character_matrix.to_csv(output_loc + 'neighbor/' + name + '_character_matrix.txt', sep='\\t')\n",
    "\n",
    "    with open(output_loc + 'neighbor/' + name + '_priors.pickle', 'wb') as handle:\n",
    "        pickle.dump(priors, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open(output_loc + 'neighbor/' + name + '_state_2_indel.pickle', 'wb') as handle:\n",
    "        pickle.dump(state_2_indel, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    cas_tree = cas.data.CassiopeiaTree(character_matrix=character_matrix, priors=priors)\n",
    "    print(cas_tree.n_cell, cas_tree.n_character, '\\n')\n",
    "    \n",
    "    missing_proportion = (character_matrix == -1).sum(axis=0) / character_matrix.shape[0]\n",
    "    uncut_proportion = (character_matrix == 0).sum(axis=0) / character_matrix.shape[0]\n",
    "    n_unique_states = character_matrix.apply(lambda x: len(np.unique(x[(x != 0) & (x != -1)])), axis=0)\n",
    "\n",
    "    cell_meta = table.groupby('cellBC').agg({\"intBC\": 'nunique', 'UMI': 'sum', 'cell_state': 'unique', 'Timepoint': 'unique', 'orig.ident':'unique'})\n",
    "    character_meta = pd.DataFrame([missing_proportion, uncut_proportion, n_unique_states], index = ['missing_prop', 'uncut_prop', 'n_unique_states']).T\n",
    "\n",
    "    cas_tree.cell_meta = cell_meta\n",
    "    cas_tree.character_meta = character_meta\n",
    "    cas_tree.cell_meta.to_csv(output_loc + 'neighbor/' + name + '_metadata.txt', sep='\\t')\n",
    "\n",
    "    cas_tree.cell_meta['cell_state'] = cas_tree.cell_meta['cell_state'].astype('str')\n",
    "    \n",
    "    nj_solver = cas.solver.NeighborJoiningSolver(dissimilarity_function=cas.solver.dissimilarity.weighted_hamming_distance, add_root=True)\n",
    "    nj_solver.solve(cas_tree, collapse_mutationless_edges=True)\n",
    "\n",
    "    newickFile = open(output_loc + 'neighbor/' + name + '_neighbor_newick.txt', 'wt')\n",
    "    newickFile.write(cas_tree.get_newick())\n",
    "    newickFile.close()\n",
    "\n",
    "    cas_tree.collapse_mutationless_edges(infer_ancestral_characters=True)\n",
    "\n",
    "    noMutationlessEdgesFile = open(output_loc + 'neighbor/' + name + '_neighbor_newick_noMutationlessEdges.txt', 'wt')\n",
    "    noMutationlessEdgesFile.write(cas_tree.get_newick())\n",
    "    noMutationlessEdgesFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07cac601",
   "metadata": {},
   "source": [
    "# Generate greedy trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ef64e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the output file locations exist\n",
    "for barcode in barcodes:\n",
    "    for time in [['120h'], ['144h'], ['120h', '144h']]:\n",
    "        if len(time) > 1:\n",
    "            timepoint = '{}_{}'.format(time[0][:-1], time[1][:-1])\n",
    "        else:\n",
    "            timepoint = '{}'.format(time[0][:-1])\n",
    "            \n",
    "        if not os.path.exists('/Genomics/chanlab/blaw/TLS/data/explant/lineage/3_lineage_reconstruction/{}/{}/greedy/'.format(barcode, timepoint)):\n",
    "            os.mkdir('/Genomics/chanlab/blaw/TLS/data/explant/lineage/3_lineage_reconstruction/{}/{}/greedy/'.format(barcode, timepoint))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2817d908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the greedy algorithm\n",
    "for barcode in barcodes:\n",
    "    for time in [['120h'], ['144h'], ['120h', '144h']]:\n",
    "        temp_tableFiltered = tableFiltered[(tableFiltered['finalCalls'] == barcode) & (tableFiltered['Timepoint'].isin(time))].copy()\n",
    "        \n",
    "        temp_tableFiltered = remove_NaN(temp_tableFiltered)\n",
    "\n",
    "        #temp_tableFiltered = remove_NaN(temp_tableFiltered)\n",
    "        if len(time) > 1:\n",
    "            timepoint = '{}_{}'.format(time[0][:-1], time[1][:-1])\n",
    "        else:\n",
    "            timepoint = '{}'.format(time[0][:-1])\n",
    "            \n",
    "        temp_tableFiltered.to_csv(output + '{}/{}/{}_{}_allele_table.txt'.format(barcode, timepoint, barcode, timepoint), sep = '\\t')\n",
    "        # I will use the best see for TLS1 for now\n",
    "        if not os.path.exists('/Genomics/chanlab/blaw/TLS/data/explant/lineage/3_lineage_reconstruction/{}/{}/greedy/'.format(barcode, timepoint)):\n",
    "            os.mkdir('/Genomics/chanlab/blaw/TLS/data/explant/lineage/3_lineage_reconstruction/{}/{}/greedy/'.format(barcode, timepoint))\n",
    "        \n",
    "        greedy_tree_pipeline(table = temp_tableFiltered,\n",
    "                             name = '{}_{}'.format(barcode, timepoint), \n",
    "                             total_priors = total_priors,\n",
    "                             output_loc = '/Genomics/chanlab/blaw/TLS/data/explant/lineage/3_lineage_reconstruction/{}/{}/'.format(barcode, timepoint))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341dbc80",
   "metadata": {},
   "source": [
    "# Generate neighbor joining trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11597071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the output file locations exist\n",
    "for barcode in barcodes:\n",
    "    for time in [['120h'], ['144h'], ['120h', '144h']]:\n",
    "        if len(time) > 1:\n",
    "            timepoint = '{}_{}'.format(time[0][:-1], time[1][:-1])\n",
    "        else:\n",
    "            timepoint = '{}'.format(time[0][:-1])\n",
    "            \n",
    "        if not os.path.exists('/Genomics/chanlab/blaw/TLS/data/explant/lineage/3_lineage_reconstruction/{}/{}/neighbor/'.format(barcode, timepoint)):\n",
    "            os.mkdir('/Genomics/chanlab/blaw/TLS/data/explant/lineage/3_lineage_reconstruction/{}/{}/neighbor/'.format(barcode, timepoint))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07125a74",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping the following intBCs due to lack of diversity with threshold 1: []\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ad763cfbd064f72a22a7703fc1dd6a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing characters:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2080 30 \n",
      "\n",
      "Dropping the following intBCs due to lack of diversity with threshold 1: []\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eab6e63f034d46358005b36f9ccb0109",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing characters:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4679 30 \n",
      "\n",
      "Dropping the following intBCs due to lack of diversity with threshold 1: []\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9b233280d854367add271e2555088cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing characters:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6759 30 \n",
      "\n",
      "Dropping the following intBCs due to lack of diversity with threshold 1: []\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c6081a13b054880ba0e3ca9b7a1c519",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing characters:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1520 30 \n",
      "\n",
      "Dropping the following intBCs due to lack of diversity with threshold 1: []\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e9dd032bfbd446a84dec16ae26caa9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing characters:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2366 30 \n",
      "\n",
      "Dropping the following intBCs due to lack of diversity with threshold 1: []\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d282d917cb2406bbe87dcb57d27d3fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing characters:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3886 30 \n",
      "\n",
      "Dropping the following intBCs due to lack of diversity with threshold 1: []\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "febf113fa09e4e64b4cf967898ab5e77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing characters:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1476 30 \n",
      "\n",
      "Dropping the following intBCs due to lack of diversity with threshold 1: []\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44602b471dc749a7ac6ed6b89fc4103b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing characters:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2879 30 \n",
      "\n",
      "Dropping the following intBCs due to lack of diversity with threshold 1: []\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d193815b17643438d224aa7c4f4370d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing characters:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4355 30 \n",
      "\n",
      "Dropping the following intBCs due to lack of diversity with threshold 1: []\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4adb80b8c3be459d9372fa6445ce96db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing characters:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "415 30 \n",
      "\n",
      "Dropping the following intBCs due to lack of diversity with threshold 1: []\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6499d5798a349a6a9915db6bf0197f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing characters:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2548 30 \n",
      "\n",
      "Dropping the following intBCs due to lack of diversity with threshold 1: []\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e5cdf8dd9db4648ae51da8ee05e8183",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing characters:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2963 30 \n",
      "\n",
      "Dropping the following intBCs due to lack of diversity with threshold 1: []\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f012b5a1b80b4e939a72ab11fd16fa79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing characters:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "311 30 \n",
      "\n",
      "Dropping the following intBCs due to lack of diversity with threshold 1: []\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7aba201f4a54038a9e32b96bed82c28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing characters:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2452 30 \n",
      "\n",
      "Dropping the following intBCs due to lack of diversity with threshold 1: []\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe30933fd71b4c39b39f51f18c9c84f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing characters:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2763 30 \n",
      "\n",
      "Dropping the following intBCs due to lack of diversity with threshold 1: []\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eab5701ebcb24757875c832d683d57c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing characters:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "243 30 \n",
      "\n",
      "Dropping the following intBCs due to lack of diversity with threshold 1: []\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a83f27d33b444d8aeee3377918bdce0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing characters:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "443 30 \n",
      "\n",
      "Dropping the following intBCs due to lack of diversity with threshold 1: []\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e84d4c773d8a42219810584adc3503c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing characters:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "686 30 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for barcode in barcodes:\n",
    "    for time in [['120h'], ['144h'], ['120h', '144h']]:\n",
    "        temp_tableFiltered = tableFiltered[(tableFiltered['finalCalls'] == barcode) & (tableFiltered['Timepoint'].isin(time))].copy()\n",
    "        \n",
    "        temp_tableFiltered = remove_NaN(temp_tableFiltered)\n",
    "\n",
    "        #temp_tableFiltered = remove_NaN(temp_tableFiltered)\n",
    "        if len(time) > 1:\n",
    "            timepoint = '{}_{}'.format(time[0][:-1], time[1][:-1])\n",
    "        else:\n",
    "            timepoint = '{}'.format(time[0][:-1])\n",
    "            \n",
    "        #temp_tableFiltered.to_csv(output + '{}/{}/{}_{}_allele_table.txt'.format(barcode, timepoint, barcode, timepoint), sep = '\\t')\n",
    "\n",
    "        NJ_tree_pipeline(table = temp_tableFiltered,\n",
    "                         name = '{}_{}'.format(barcode, timepoint), \n",
    "                         total_priors = total_priors,\n",
    "                         output_loc = '/Genomics/chanlab/blaw/TLS/sandbox/explant/3_lineage_reconstruction/{}/{}/'.format(barcode, timepoint))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
